# Kolo

**Kolo** is a lightweight tool designed for **fast and efficient fine-tuning and testing of Large Language Models (LLMs)** on your local machine. It leverages cutting-edge tools to simplify the fine-tuning process, making it as quick and seamless as possible.

## ğŸš€ Features

- ğŸ— **Lightweight**: Minimal dependencies, optimized for speed.
- âš¡ **Runs Locally**: No need for cloud-based services; fine-tune models on your own machine.
- ğŸ›  **Easy Setup**: Simple installation and execution with Docker.
- ğŸ”Œ **Support for Popular Frameworks**: Integrates with major LLM toolkits.

## ğŸ›  Tools Used

Kolo is built using a powerful stack of LLM tools:

- [Unsloth](https://github.com/unslothai/unsloth) â€“ Efficient fine-tuning for LLMs.
- [Hugging Face](https://huggingface.co/) â€“ Model hosting, training, and deployment.
- [Llama.cpp](https://github.com/ggerganov/llama.cpp) â€“ Fast inference for Llama models.
- [Ollama](https://ollama.ai/) â€“ Simple and portable model management.
- [Docker](https://www.docker.com/) â€“ Containerized environment for easy deployment.

## ğŸƒ Getting Started

### 1ï¸âƒ£ Install Docker

Ensure [Docker](https://docs.docker.com/get-docker/) is installed on your system.

### 2ï¸âƒ£ Build the Image

`./build.ps1`

### 3ï¸âƒ£ Run the Container

`./run.ps1`

### 4ï¸âƒ£ Connect via SSH

`./connect.ps1`

`password 123`

### 5ï¸âƒ£ Copy over your JSONL dataset

...

### 6ï¸âƒ£ Train

...

### 7ï¸âƒ£ Run

...
